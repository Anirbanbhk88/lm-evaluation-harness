"""
Adversarial NLI: A New Benchmark for Natural Language Understanding
https://arxiv.org/pdf/1910.14599.pdf

Adversarial NLI (ANLI) is a dataset collected via an iterative, adversarial
human-and-model-in-the-loop procedure. It consists of three rounds that progressively
increase in difficulty and complexity, and each question-answer includes annotator-
provided explanations.

Homepage: "https://github.com/facebookresearch/anli"
"""
import numpy as np
from lm_eval.base import rf, Task
from lm_eval.metrics import mean


_CITATION = """
@inproceedings{nie-etal-2020-adversarial,
    title = "Adversarial {NLI}: A New Benchmark for Natural Language Understanding",
    author = "Nie, Yixin  and
      Williams, Adina  and
      Dinan, Emily  and
      Bansal, Mohit  and
      Weston, Jason  and
      Kiela, Douwe",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    year = "2020",
    publisher = "Association for Computational Linguistics",
}
"""


class ANLIDE(Task):
    VERSION = 0
    DATASET_PATH = "MoritzLaurer/multilingual-NLI-26lang-2mil7"
    DATASET_NAME = "de_anli"

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return False

    def has_test_docs(self):
        return False

    def training_docs(self):
        if self.has_training_docs():
            # We cache training documents in `self._training_docs` for faster
            # few-shot processing. If the data is too large to fit in memory,
            # return the training data as a generator instead of a list.
            if self._training_docs is None:
                # TODO: Return the training document generator from `self.dataset`.
                # If you need to process the data, `map` over the documents with
                # the custom processing function, `self._process_doc`. E.g.
                # `map(self._process_doc, self.dataset["validation"])`
                # In most case you can leave this as is unless the dataset split is
                # named differently than the default `"train"`.
                self._training_docs = map(self._process_doc, self.dataset["de_anli"])
            return self._training_docs
        # return self.dataset["de_anli"]
        # if self.has_training_docs():
        #     return self.dataset["de_anli" + str(self.SPLIT)]
        # if self.has_training_docs():
        # if self._training_docs is None:
        # self._training_docs = list(self.dataset["de_anli"])
        # return self._training_docs

    # def doc_to_text(self, doc):
    #     # OA does this a bit weirdly: they prepend "anli 1:  anli 1:  " to the beginning
    #     # of the prompt (yes, repeating it!). also, " True, False, or Neither?" is directly
    #     # appended onto the question, with no "Answer:" or even a newline. Do we *really*
    #     # want to do it exactly as OA did?
    #     return (
    #         doc["premise"]
    #         + "\nFrage: "
    #         + doc["hypothesis"]
    #         + " Richtig, Falsch oder Keins von beiden?\nAntwort:"
    #     )

    def doc_to_text(self, doc):
        return (
            "PrÃ¤misse: {}\nHypothese: {} Wahr, Falsch oder Neutral?\nAntwort:".format(
                doc["premise"],
                doc["hypothesis"].strip()
                + ("" if doc["hypothesis"].strip().endswith(".") else "."),
            )
        )

    # def doc_to_decontamination_query(self, doc):
    #    return doc["premise"]

    # def doc_to_target(self, doc):
    #     # True = entailment
    #     # False = contradiction
    #     # Neither = neutral
    #     return (" " + ["Richtig", "Keins von beiden", "Falsch"][str(doc["label"])])

    # def doc_to_target(self, doc):
    #     return " {}".format({0: "Richtig", 1: "Keins von beiden", 2:"Falsch"}[doc["label"]])

    def doc_to_target(self, doc):
        # True = entailment
        # False = contradiction
        # Neither = neutral
        return " {}".format({0: "Wahr", 1: "Neutral", 2: "Falsch"}[doc["label"]])

    # def construct_requests(self, doc, ctx):
    #     """Uses RequestFactory to construct Requests and returns an iterable of
    #     Requests which will be sent to the LM.

    #     :param doc:
    #         The document as returned from training_docs, validation_docs, or test_docs.
    #     :param ctx: str
    #         The context string, generated by fewshot_context. This includes the natural
    #         language description, as well as the few shot examples, and the question
    #         part of the document for `doc`.
    #     """
    #     ll_true, _ = rf.loglikelihood(ctx, " Richtig")
    #     ll_neither, _ = rf.loglikelihood(ctx, " Keins von beiden")
    #     ll_false, _ = rf.loglikelihood(ctx, " Falsch")
    #     return ll_true, ll_neither, ll_false

    def construct_requests(self, doc, ctx):
        ll_true, _ = rf.loglikelihood(ctx, " Wahr")
        ll_neither, _ = rf.loglikelihood(ctx, " Neutral")
        ll_false, _ = rf.loglikelihood(ctx, " Falsch")
        return ll_true, ll_neither, ll_false

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        gold = str(doc["label"])
        pred = np.argmax(results)
        # pred = ["Richtig", "Keins von beiden", "Falsch"][pred_label]
        # print(pred, gold)
        return {"acc": pred == gold}

    def aggregation(self):
        """
        :returns: {str: [float] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metrics
        """
        return {"acc": mean}

    def higher_is_better(self):
        """
        :returns: {str: bool}
            A dictionary where keys are the names of submetrics and values are
            whether a higher value of the submetric is better
        """
        return {"acc": True}
